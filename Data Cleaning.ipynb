{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da150e20-7d6f-4372-8850-74afafbff021",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Cleaning (not lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb95c28-9560-4cd1-b3ed-01ae256d8351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/paula/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efaf2a3a-c400-4f49-882b-3284b3704f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (41157, 6)\n",
      "Test shape :  (3798, 6)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('Corona_NLP_train.csv',encoding='latin1')  \n",
    "test = pd.read_csv('Corona_NLP_test.csv',encoding='latin1')  \n",
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b4eb190-e13f-4b72-aef2-fbb62be1d103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44955"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "41157+3798"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3a7c72-448a-417d-9413-a161f2a3facc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-05T11:35:57.748569Z",
     "iopub.status.busy": "2021-12-05T11:35:57.747932Z",
     "iopub.status.idle": "2021-12-05T11:35:57.753489Z",
     "shell.execute_reply": "2021-12-05T11:35:57.752685Z",
     "shell.execute_reply.started": "2021-12-05T11:35:57.748528Z"
    }
   },
   "source": [
    "#### Divide into three Sentiment Classes\n",
    "Encode the categories with numbers and create three sentiment categories: Positive, Neutral and Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc48b797-b856-4c0c-be36-8d58b4d6042c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['Sentiment'] = train['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})\n",
    "test['Sentiment'] = test['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca655018-b349-412e-aa7f-905dc5bca403",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create function for pre-processing\n",
    "The lemmatizer of this function is  specifically utilized for the data that is input to models employing count-based techniques for feature extraction (BoW, PoS, Tf-Idf). The \"data\" directory includes the non-lemmatized data, while the \"data lemmatized\" directory includes the same data that has been lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9fbfc2a-7928-4574-8faa-e5aa6d390acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Clean the Tweet column\n",
    "def clean_tweet(input_tweets): \n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tweets = input_tweets.str.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweets = tweets.replace(r\"http\\S+\", \" \", regex=True)\n",
    "    tweets = tweets.replace(r\"http\", \" \", regex=True)\n",
    "    \n",
    "    # Remove usernames\n",
    "    tweets = tweets.replace(r\"@\\S+\", \" \", regex=True)\n",
    "    \n",
    "    # Replace \"@\" with \"at\"\n",
    "    tweets = tweets.replace(r\"@\", \"at\", regex=True)\n",
    "    \n",
    "    # Remove non-alphanumeric characters except apostrophes and spaces\n",
    "    tweets = tweets.replace(r\"[^\\w\\s']\",\" \", regex=True)\n",
    "    \n",
    "    # Remove non-ASCII characters\n",
    "    tweets = tweets.replace(r'[^\\x00-\\x7F]+',\" \", regex=True)\n",
    "    \n",
    "    # Remove digits\n",
    "    tweets = tweets.replace(r'\\d+',\"\", regex=True)\n",
    "    \n",
    "    # Tokenize the tweets\n",
    "    #tweets = tweets.apply(lambda x: re.findall(r'\\b\\w+\\b', x))\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #tweets = tweets.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    \n",
    "    # Join the lemmatized tokens back into strings\n",
    "    #tweets = tweets.apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712ff46e-53da-43fa-9cb8-8f8a971c75df",
   "metadata": {},
   "source": [
    "#### Clean tweets and remove stopwords\n",
    "Apply the `clean_tweet` function to the original tweets, add the clean tweets to a new column and remove nltk stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ff6c55-1c07-4730-9431-169169531187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place clean data in new column \n",
    "train['CleanTweet'] = clean_tweet(train['OriginalTweet'])\n",
    "test['CleanTweet'] = clean_tweet(test['OriginalTweet'])\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords\n",
    "train['CleanTweet'] = train['CleanTweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "test['CleanTweet'] = test['CleanTweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82577a8d-f331-4ff2-842e-a934a92cde40",
   "metadata": {},
   "source": [
    "#### Remove irrelevant columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37e88663-5458-4843-88c5-f400be93f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns which we don't need\n",
    "train = train.drop(['OriginalTweet','UserName','ScreenName','Location','TweetAt'], axis = 1)\n",
    "test = test.drop(['OriginalTweet','UserName','ScreenName','Location','TweetAt'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c6b9f5f-d1d4-46d2-af22-3f0e92d91a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (41157, 2)\n",
      "Test shape :  (3798, 2)\n"
     ]
    }
   ],
   "source": [
    "# Current training set and test set shape\n",
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd614ce-89f5-427f-81ae-ac9b801f82eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (41130, 2)\n",
      "Test shape :  (3796, 2)\n"
     ]
    }
   ],
   "source": [
    "# Remove non-string values and empty string values\n",
    "train = train[train['CleanTweet'].apply(lambda x: isinstance(x, str) and x != '')]\n",
    "test = test[test['CleanTweet'].apply(lambda x: isinstance(x, str) and x != '')]\n",
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)\n",
    "# Removed 17 values that are non-string or empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "104515c7-1281-4d94-ba77-96db252b7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a087fe0-d3ab-489c-8b88-1e081d9d4433",
   "metadata": {},
   "source": [
    "#### Split into 80% Train, 10% Validation, and 10% Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d017121b-d261-44c0-ab56-5ee43748f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set 90%\n",
    "X_test = test[\"CleanTweet\"]\n",
    "y_test = test[\"Sentiment\"]\n",
    "\n",
    "# Train set 10%\n",
    "X_train = train[\"CleanTweet\"]\n",
    "y_train = train['Sentiment']   \n",
    "\n",
    "# Further split X_train into 80% Train and 10% Validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=37334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da4e1e7-20c4-4923-9913-b10d3f39d8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37334\n",
      "37334\n",
      "3796\n",
      "3796\n",
      "3796\n",
      "3796\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train)) \n",
    "print(len(y_train))\n",
    "\n",
    "print(len(X_val))  \n",
    "print(len(y_val))\n",
    "\n",
    "print(len(X_test)) \n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcc9b7e-3d2e-4f13-b312-801c695986bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d1acf0-8297-4dce-a480-3ff5f0bd1950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training data \n",
    "X_train.to_csv(\"data/X_train.csv\")\n",
    "y_train.to_csv(\"data/y_train.csv\")\n",
    "\n",
    "# Save the validation data \n",
    "X_val.to_csv(\"data/X_val.csv\")\n",
    "y_val.to_csv(\"data/y_val.csv\")\n",
    "\n",
    "# Save the test data \n",
    "X_test.to_csv(\"data/X_test.csv\")\n",
    "y_test.to_csv(\"data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac347d-92d3-4c93-b92e-c37163ddd42d",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f35586f6-0f7e-48d1-b9bf-8cb19697ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data \n",
    "X_train = pd.read_csv(\"data/X_train.csv\", index_col=0).reset_index(drop=True)['CleanTweet']\n",
    "y_train = pd.read_csv(\"data/y_train.csv\", index_col=0).reset_index(drop=True)['Sentiment']\n",
    "\n",
    "# Load the validation data \n",
    "X_val = pd.read_csv(\"data/X_val.csv\", index_col=0).reset_index(drop=True)['CleanTweet']\n",
    "y_val = pd.read_csv(\"data/y_val.csv\", index_col=0).reset_index(drop=True)['Sentiment']\n",
    "\n",
    "# Load the test data \n",
    "X_test = pd.read_csv(\"data/X_test.csv\", index_col=0).reset_index(drop=True)['CleanTweet']\n",
    "y_test = pd.read_csv(\"data/y_test.csv\", index_col=0).reset_index(drop=True)['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e9b2f-2766-44e2-815e-b1cf6b003eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27f72ff5-77f8-4adf-9767-056a560297cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305e218-b6ca-4546-a3a2-9e8805f19a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10612bfb-5b11-4f76-9aac-a2af4b602d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
